1. open Terminal

install Homebrew --> install Hadoop --> install Java -->install scalar 

2. install spark from Homebrew
$brew install apache-spark

3. specify path, be aware of the version of your Spark, the sample shows version of 2.3.0
$ export SPARK_HOME=/usr/local/Cellar/apache-spark/2.3.0/libexec
$ export PYTHONPATH=/usr/local/Cellar/apache-spark/2.3.0/libexec/python/:$PYTHONP$

4. start spark 
$ spark-shell

5. the start interface will be as follows

Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)
Type in expressions to have them evaluated.
Type :help for more information.

6. test the follow sample code in scala to see if spark works properly

Word count

val textFile = sc.textFile("hdfs://...")
val counts = textFile.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...")


